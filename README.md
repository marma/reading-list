# Reading list

## The Unreasonable Effectiveness of Recurrent Neural Networks

"There’s something magical about Recurrent Neural Networks (RNNs). I still remember when I trained my first recurrent network for Image Captioning. Within a few dozen minutes of training my first baby model (with rather arbitrarily-chosen hyperparameters) started to generate very nice looking descriptions of images that were on the edge of making sense. Sometimes the ratio of how simple your model is to the quality of the results you get out of it blows past your expectations, and this was one of those times. What made this result so shocking at the time was that the common wisdom was that RNNs were supposed to be difficult to train (with more experience I’ve in fact reached the opposite conclusion). Fast forward about a year: I’m training RNNs all the time and I’ve witnessed their power and robustness many times, and yet their magical outputs still find ways of amusing me. This post is about sharing some of that magic with you."

[http://karpathy.github.io/2015/05/21/rnn-effectiveness/]


http://karpathy.github.io/2015/05/21/rnn-effectiveness/
https://arxiv.org/abs/1706.03762
https://magenta.tensorflow.org/music-transformer
https://arxiv.org/pdf/1905.05583.pdf
https://arxiv.org/pdf/1210.0999.pdf
https://dl.acm.org/citation.cfm?doid=2960811.2967165
https://www.semanticscholar.org/paper/Fully-Convolutional-Neural-Networks-for-Newspaper-Meier-Stadelmann/7b12746f5af6f473f9ed7685e3eac199077ce205
https://ufal.mff.cuni.cz/pbml/110/art-popel-bojar.pdf
https://arxiv.org/abs/1904.12848
https://blog.inten.to/speeding-up-bert-5528e18bb4ea
https://arxiv.org/abs/1909.10351
https://towardsdatascience.com/the-most-important-supreme-court-decision-for-data-science-and-machine-learning-44cfc1c1bcaf

